\section{Linear Regression}

\subsection{Problem Statement}

\begin{definition}[Linear Regression Problem]
Given a training set $D = \{x_i, y_i\}_{i=1}^N$ where:
\begin{itemize}
    \item Each $x_i$ is a $D$-dimensional real-valued column vector: $x = (x_1, \ldots, x_D)^\top$
    \item Each $y_i$ is a real number
\end{itemize}

Our goal is to learn a function:
\[ y = f(x, w) = w^\top x = \sum_{j=0}^D w_j x_j \]

The weights $w = (w_0, w_1, \ldots, w_D)^\top$ determine how important the features $(x_1, \ldots, x_D)$ are in predicting the response $y$.

Always set $x_0 = 1$, and $w_0$ is the bias term, often denoted by $b$.
\end{definition}

\begin{example}[Applications of Linear Regression]
Here are several examples from real-world datasets:
\begin{itemize}
    \item Predict brain weight of mammals based on their body weight
    \item Predict blood fat content based on age and weight
    \item Predict death rate from cirrhosis based on a number of other factors
    \item Predict selling price of houses based on a number of factors
\end{itemize}
\end{example}

\subsection{Mean Squared Error}

How do we determine the weights $w$? We want the predicted response values $f(x_i)$ to be close to the observed response values. So we minimize the following objective function:

\begin{definition}[Mean Squared Error]
\[ L(w) = \frac{1}{N} \sum_{i=1}^N (y_i - w^\top x_i)^2 \]
This is called the \textbf{mean squared error (MSE)}.
\end{definition}

\begin{remark}
As a function of the weights $w$, MSE is a quadratic ``bowl'' with a unique minimum. We can minimize it by setting its gradient to zero: $\nabla L(w) = 0$.
\end{remark}

\subsection{Matrix Representation}

We can represent the training set $D = \{x_i, y_i\}_{i=1}^N$ using the design matrix $X$ and a column vector $y$:

\[
X = \begin{bmatrix}
x_{1,0} & x_{1,1} & x_{1,2} & \cdots & x_{1,D} \\
x_{2,0} & x_{2,1} & x_{2,2} & \cdots & x_{2,D} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_{N,0} & x_{N,2} & x_{N,2} & \cdots & x_{N,D}
\end{bmatrix} = \begin{bmatrix}
x_1^\top \\
x_2^\top \\
\vdots \\
x_N^\top
\end{bmatrix}, \quad
y = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix}
\]

Then the MSE can be written as:
\[ L(w) = \frac{1}{N} \|y - Xw\|_2^2 = \frac{1}{N} (y - Xw)^\top(y - Xw) = \frac{1}{N} (w^\top(X^\top X)w - 2w^\top(X^\top y) + y^\top y) \]

\subsection{The Normal Equation}

\begin{theorem}[Ordinary Least Squares Solution]
From the matrix representation, we get that:
\[ \nabla L(w) = \frac{1}{N}(2X^\top Xw - 2X^\top y) \]

Setting the gradient to zero, we get the \textbf{normal equation}:
\[ X^\top Xw = X^\top y \]

The value of $w$ that minimizes $L(w)$ is:
\[ \hat{w} = (X^\top X)^{-1}X^\top y \]

This is called the \textbf{ordinary least squares (OLS)} solution.
\end{theorem}

\section{Polynomial Regression}

\subsection{Beyond Linear Regression}

Here again is the linear regression model:
\[ y = f(x) = w^\top x = \sum_{j=0}^D w_j x_j \]

Linear regression can be made to model non-linear relationships by replacing $x$ with some non-linear function of the inputs, $\phi(x)$. That is, we use:
\[ y = f(x) = w^\top \phi(x) \]

This is known as \textbf{basis function expansion} and $\phi$ is called \textbf{feature mapping}.

\subsection{Polynomial Feature Mapping}

\begin{definition}[Polynomial Regression]
For $x = [1, x_1, x_2]^\top$, we can use the following polynomial feature mapping:
\[ \phi(x) = [1, x_1, x_2, \ldots, x_D, x_1^2, x_1x_2, \ldots, x_D^d]^\top \]

When $d = 2$, we get:
\[ y = w^\top \phi(x) = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_1x_2 + w_5x_2^2 \]
\end{definition}

\begin{remark}[Model Selection]
What degree $d$ should we choose? What is the impact of $d$? This is a fundamental question in model selection.
\begin{itemize}
    \item Higher $d$ means higher model capacity
    \item Higher model capacity implies better fit to training data
    \item But does it generalize better to unseen data?
\end{itemize}
\end{remark}

\begin{example}[Polynomial Regression with Different Degrees]
Consider polynomial regression with $d = 14$ and $d = 20$ and one feature $x$. Both models can fit the training data very well, but they may have very different behavior on test data.
\end{example}
