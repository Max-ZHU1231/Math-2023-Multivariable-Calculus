\section{Probabilistic Formulation of Linear Regression}

\begin{remark}[Note]
This section is typically covered after Lecture 02 and assumes knowledge of probability theory and maximum likelihood estimation.
\end{remark}

\subsection{Probabilistic Model}

Next, we show that least squares regression can be derived from a probabilistic model.

\begin{definition}[Linear Regression with Gaussian Noise]
We assert:
\[ y = w^\top x + \epsilon = \sum_{j=0}^D w_j x_j + \epsilon \]

where the error term $\epsilon$ captures unmodeled effects and random noise.

We also assume that $\epsilon$ follows the Gaussian distribution with zero mean and variance $\sigma^2$:
\[ \epsilon \sim \mathcal{N}(0, \sigma^2) \]

The model parameters $\theta$ include $w$ and $\sigma$. The conditional distribution of $y$ given input $x$ and parameters $\theta$ is a Gaussian:
\[ p(y|x, \theta) = \mathcal{N}(y|\mu(x), \sigma^2) \]
where $\mu(x) = w^\top x$.
\end{definition}

\begin{remark}[Geometric Interpretation]
For each input $x$, we get a distribution of $y$, which is a Gaussian distribution. To get a point estimation of $y$, we can use the mean:
\[ \hat{y} = \mu(x) = w^\top x \]
\end{remark}

\subsection{Parameter Estimation}

\begin{theorem}[Maximum Likelihood Estimation]
Determine $\theta = (w, \sigma)$ by minimizing the cross entropy:
\begin{align*}
&-\frac{1}{N} \sum_{i=1}^N \log p(y_i | x_i, \theta) \\
&= -\frac{1}{N} \sum_{i=1}^N \log\left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - w^\top x_i)^2}{2\sigma^2}\right) \right] \\
&= \frac{1}{2}\log(2\pi\sigma^2) + \frac{1}{2N\sigma^2} \sum_{i=1}^N (y_i - w^\top x_i)^2
\end{align*}

Assume $\sigma$ is fixed. This is the same as minimizing the MSE:
\[ L(w) = \frac{1}{N} \sum_{i=1}^N (y_i - w^\top x_i)^2 \]
\end{theorem}

\begin{remark}[Summary]
Under some assumptions, least-squares regression can be justified as a very natural method that minimizes cross entropy, or equivalently, maximizes likelihood.

This probabilistic perspective provides:
\begin{itemize}
    \item A principled way to derive the objective function
    \item A framework for uncertainty quantification
    \item A basis for Bayesian approaches to regression
\end{itemize}
\end{remark}
